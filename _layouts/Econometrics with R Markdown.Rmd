---
Title: "R Project - Econometrics"
Author:  "Ivan Perez 21202053"
Date: "14/11/2021"
output: 
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# INDEX

1.Introduction

2.Theory tasks

3.Empirical analysis

4.Monte Carlo simulation


# 1.INTRODUCTION

The aim of the present project is to develop a theory and empirical analysis using R as a programming language. To do so, we will answer the tasks given in the work guideline. The project structure is straight forward:
Firstly, we  will answer the theory tasks assigned in the guidelines, using the knowledge that we have acquired along the course.
Secondly, using the dataset: “data_giulietti_etal.dta”, that quantifies the extent of racial discrimination in local public services in the U.S, we will work on the tasks assinged in the project guidelines. It is important to consider that, although there are many different ways to answer the questions given, we have decided to answer them in such a way that the output given is the most clear to interpret. Notice that, with the aim of distinguishing between the write-up assignment from the code we have decided that the best idea is that the code is given in a green color.
Finally, there will be a brief subsection that will work with the monte carlo simulation.

# 2. THEORY TASKS
Suppose you want to quantify the extent of discrimination in public service provision in the U.S.. Your company has collected data on email queries of citizens to public offices (libraries, police, registry), whereby citizens were asked in a survey to provide the relevant information. In particular, the survey includes data on 1) a person’s race – whether the person is white or belongs to a minority (Black or Latinx), and 2) data on how many hours it takes for a public office to respond.

**1.**Suppose you want to estimate the effect of minority status (i.e. a dummy that equals one if a person belongs to a minority – either Black or Latinx – and zero if the person is white) on the response time to an email. Write down a regression equation that would allow you to estimate this effect.

Dummy variables are useful to include qualitative information into empirical analysis. In order to estimate this effect we could use the following regression equation:
Yi= ß0 +  ß1 Xi + Ui, where Xi is the binary variable ( Xi= 1 if the person belongs to a minority and Xi= 0 if the person is white ), Yi quantifies  the response time to an email, and Ui is the error term. Notice that we are not in the case in which we have a binary dependent variable ( i.e. does a consumer buy a product or not).

**2.**Explain what parameter you are interested in estimating and provide an interpretation of this parameter.
We are interested in the parameter ß1, which is the slope of the regression line but also the difference in means of two groups: 

ß1= E(Yi/Xi=1) - E( Yi/Xi=0)
 By estimating ß1 we can quantify up to what extent there is an effect on being part of a minority status that affects the time of the response of an email.

**3.**Discuss the random sampling assumption and the conditional independence assumption (in the lecture it was E(u|X) = 0). Are these assumptions fulfilled in this case (explain why or why not)? Explain intuitively the likely consequences of these assumptions (not) being fulfilled for estimating the effect of interest.

The Conditional independence assumption is fulfilled since there are no external factors (u) that could affect the race or minority of someone (X) and the same occurs with the random sampling assumption. The data is collected from the same population, then it is identically distributed, and we assume the surveys were taken randomly, so the values are also independently distributed.
Since these assumptions are fulfilled, we can get an estimate very close to the actual one of the population. If not, the result could be biased by the u thus the sampling β1 has 0 approximation to the real β1.

**4.**If you could run an experiment (regardless of ethical considerations) to estimate the effect of interest, what would this experiment look like and why? (N.B.: the ideal experiment asked for here is different from an experiment described further below.)

The ideal experiment for this case would be one based on surveys conducted randomly on people of both different races and then compare the average number of hours it took to be answered by the public office, thus arriving at the true result of whether there is discrimination in the public sector.

**3. EMPIRICAL ANALYSIS**

Packages needed
```{r}
library(tidyr)
library(dplyr)
library(gmodels)
library(tidyverse)
library(epiDisplay)
library(stargazer)
library(ggplot2)
library(haven) #for read_data 
data <- read_dta("C:/Users/ivvan/Downloads/data_giulietti_etal.dta")
data1<-as.data.frame( read_dta("C:/Users/ivvan/Downloads/data_giulietti_etal.dta"))
```


**1.** Load the dataset into R and produce a table of summary statistics (number of observations, mean, sd, median, min, max, number of missing observations) for the variables listed in Table 1. Interpret the mean of reply, complexity,cordial_reply, length_reply and race.

```{r 1}
data= data.frame(data)

stargazer(data[c("reply", "complexity", "number_reply", "cordial_reply", "length_reply", "race")], type="text",
            summary.stat=c("mean", "sd", "median", "min", "max", "N"))

Firstexercise = stargazer(data[c("reply", "complexity", "number_reply", "cordial_reply", "length_reply", "race")], type="text",
                                                     summary.stat=c("mean", "sd", "median", "min", "max", "N"))

#answering the question subseting the data

newdata= subset(data, select =c(reply, complexity, cordial_reply, length_reply, race))

na_count = sapply(newdata, function(y) sum(length(which(is.na(y)))))

stargazer(newdata, type="text",
          summary.stat=c("mean", "sd", "median", "min", "max", "N"), na_count)

```

As we can observe we have, on the one hand, the summary statistics regarding the variables that we are interested in ( number of observations, mean, sd, median, min, max) and, on the other hand, the number of missing information.
Finally, according to the information regarding the mean of the variables, we can deduce:
There are more emails that received a reply than emails that have not, as the dummy variable mean was over 0,5. Notice that we can´t say that 66.5% of the email has been answered, because we are not talking about a continuous variable, but a dummy variable.

There are more complex emails than no complex emails, as the dummy variable was over 0,5. Notice that we can´t say that 70.5% of the emails are cordial, because we are not talking about a continuous variable, but a dummy variable.
 
There are more cordial replies than no cordial replies, as the dummy variable was over 0,5.  Notice that we can´t say that 44.9% of the emails are complex, because we are not talking about a continuous variable, but a dummy variable.

The mean for the length of replies is 175.345 words per email . There are more white than black, as the dummy variable was under 0,5.  Notice that we can´t say that 49.7% of the emails are sent by white, because we are not talking about a continuous variable, but a dummy variable. It is important to take into account that the mean is very close to 50% as the aim of the project is to a quantifies the extent of racial discrimination in local public services in the U.S, therefore it is important that there is an equilibrium between black and white sender.


**2.** Produce a frequency table for the number of emails sent to each type of recipient. The table should include the number as well as the share of emails sent to each recipient.

```{r 2}
library(epiDisplay)
tab1(data$recipient, sort.group = "decreasing", cum.percent = TRUE, main = "Number of answers to each recipient", ylab = "number of emails")
Secondexercise= tab1(data$recipient, sort.group = "decreasing", cum.percent = TRUE, main = "Number of answers to each recipient", ylab = "number of emails")
```

As we can observe in the frequency column we indicate the number of emails that each recipient has received, in the percentage column we analyze the percentage of emails received by each recipient in relation to the rest and, in the cumulative percentage column we demonstrate how the sum of the percentage of all recipients is equal to 100%. 

So, according to our analysis where the number assigned to each recipient stands for: 1: school district, 2: library, 3: sheriff, 4: treasurer, 5: job center, 6: county clerk, there is a clear domination of emails sent to the school district recipient, whereas the county clerk is clearly not as popular among the email senders.


**3.** Produce a frequency table with senders on the horizontal and recipients on the vertical axis. Each cell should show the share of all emails that was sent by a given sender to a given recipient (hint: search for cross tabulation). What does the result tell you about the quality of the randomisation in the experiment?

```{r 3}
library(gmodels)

CrossTable(data$recipient, data$sender, prop.t=TRUE, prop.r=FALSE, prop.c=FALSE, prop.chisq = FALSE, digits = 6)
```

As we can observe in the following table, in each cell we have two different numbers: N (number of total observations) and the percentage of each combination ( N / table total). We also have information regarding the column total and the row total. 
These results can say much of the quality of  the randomisation in the experiment, in fact as the percentage of each cell is similar in every row, this means that on average each recipient receives a similar number of emails from each sender. This makes sense as the aim of the  experiment is to quantify the extent of racial discrimination in local public services in the U.S , therefore each sender must send very similar number of emails to each recipient in order to quantify this discrimination. If each kind of sender only sent emails to specific recipients and not to all of them, then the experiment would not make much sense .


**4.** Run t-tests comparing the difference in means between Whites and Blacks for the following variables:
reply, cordial_reply, length_reply, delay_reply. The results of the t-tests should be presented in a table that shows the following: each row is a variable; columns: mean of the variable for Whites, mean of variable for Blacks, difference in means between Whites and Blacks, p-value of t-test. Interpret your findings regarding magnitude and statistical significance.

```{r 4}
reply= subset(data, select =c(reply))

race= subset(data, select =c(race))

length_reply= subset(data, select =c(length_reply))

cordial_reply= subset(data, select =c(cordial_reply))

delay_reply= subset(data, select =c(delay_reply))

#once we have done an object for each, now we run t.test 


 table1= t.test(reply[race==1], reply[race==0])
 table2= t.test(delay_reply[race==1], delay_reply[race==0])
 table3= t.test(cordial_reply[race==1], cordial_reply[race==0])
 table4= t.test(length_reply[race==1], length_reply[race==0])
 
 CPV= table1$p.value
 CMeans= table1$estimate
 
   RPV= table2$p.value
 RMeans= table2$estimate
 
   LPV= table3$p.value
 LMeans= table3$estimate
 
 DPV= table4$p.value
 DMeans= table4$estimate
 
   
   Tabledata = data.frame( 
         row.names = c("reply", "delay_reply", "cordial_reply", "length_reply"),
         Mean_white = c(CMeans[1:1],  RMeans[1:1], LMeans[1:1], DMeans[1:1]),
         Mean_black = c(CMeans[2:2],  RMeans[2:2], LMeans[2:2], DMeans[2:2]),
         Diference_means = c(CMeans[1:1]-CMeans[2:2],  RMeans[1:1]-RMeans[2:2], 
                                                       LMeans[1:1]-LMeans[2:2], DMeans[1:1]-DMeans[2:2]),
         p_value = (c(CPV, RPV, LPV, DPV))
 )
 stargazer(Tabledata,type='text',title = "Exercise 4 solution", summary = FALSE)
```

With this input we obtain can create a chart that contains information regarding the mean of the variable for Whites, mean of variable for Blacks, difference in means between Whites and Blacks, p-value of t-test.

According to the following table, and taking into account that the reference group is x= 0, this is the white group, we can observe that: Although more emails where replied on average to the black group, it is also true that the delay of the reply was larger for them, in detail, 0,816 hours more on average. Additionally, the length of the reply to the email is on average larger for white than for black senders, in detail, 0.308 more words on average. Nevertheless, there were more cordial replies on average for the black group than for the white group.
Finally, it is key to understand that as the p-value of reply and cordial_reply are so low, we have approximated it too but, in fact, they are 3.27e-14 for reply and 1.25e-22 for cordial_reply.


**5.** Regress the dummy reply on the dummy race. Interpret the coefficients of the slope and intercept, comment on statistical significance, and compare your results to those in the table produced in 4.

```{r 5}
reg5 <- lm(reply ~ race, data=data)
stargazer(reg5, type="text")
summary(reg5)

```

In this regression line we have a negative intercept. It means that if race is black (0) the reply would be negative, which it makes no sense, it is not possible to have negative replies. Then the β1 is 0.683, due to being less in 1, it shows that a change from 0 to 1 in race would mean a change in the probability of receiving a reply of 68.3%. Eventually, we have a very strong statistical significance because the p-value is a very small number, meaning that the difference of replies received between races is statistically significant.

**6.** Another way of analysing the results of an experiment like this is through bar charts with error bars.
You plot the means for the treatment and control group and attach to each bar a so-called error bar(y ± sd(y)). The error bars give an indication of the variation in each group. Produce such a chart (separate bars for Black and White) for the following outcomes: reply, cordial_reply, length_reply.

```{r 6}
library(haven)
library(ggplot2)
data$reply <- as.factor(data$reply)
data$cordial_reply <- as.factor(data$cordial_reply)
data$length_reply <- as.factor(data$length_reply)
head(data)

data_summary <- function(data, varname, groupnames){
  require(plyr)
  summary_func <- function(x, col){
    c(mean = mean(x[[col]], na.rm=TRUE),
      sd = sd(x[[col]], na.rm=TRUE))
  }
  data_sum<-ddply(data, groupnames, .fun=summary_func,
                  varname)
  data_sum <- rename(data_sum, c("mean" = varname))
 return(data_sum)
}

  # 1

summary6.1  <- data_summary(data1, varname="reply", 
                    groupnames=c("race", "race"))

summary6.1$race=as.factor(summary6.1$race)

p<- ggplot(summary6.1 , aes(x=race, y=reply, fill=race)) + 
  geom_bar(stat="identity", color="black", 
           position=position_dodge()) +
  geom_errorbar(aes(ymin=reply-sd, ymax=reply+sd), width=.2,
                 position=position_dodge(.9)) 
p+labs(title="Reply", x="race", y = "Reply")+
   theme_classic() +
   scale_fill_manual(values=c('#2c19d4','#d41948'))


  # 2

summary6.2 <- data_summary(data1, varname="cordial_reply", 
                    groupnames=c("race", "race"))

summary6.2$race=as.factor(summary6.2$race)

p<- ggplot(summary6.2 , aes(x=race, y=cordial_reply, fill=race)) + 
  geom_bar(stat="identity", color="black", 
           position=position_dodge()) +
  geom_errorbar(aes(ymin=cordial_reply-sd, ymax=cordial_reply+sd), width=.2,
                 position=position_dodge(.9)) 
p+labs(title="cordial_reply", x="race", y = "cordial_Reply")+
   theme_classic() +
   scale_fill_manual(values=c('#2c19d4','#d41948'))

  # 3 
summary6.3  <- data_summary(data1, varname="length_reply", 
                    groupnames=c("race", "race"))

summary6.3$race=as.factor(summary6.3$race)

p<- ggplot(summary6.3 , aes(x=race, y=length_reply, fill=race)) + 
  geom_bar(stat="identity", color="black", 
           position=position_dodge()) +
  geom_errorbar(aes(ymin=length_reply-sd, ymax=length_reply+sd), width=.2,
                 position=position_dodge(.9)) 
p+labs(title="length_reply", x="race", y = "length_reply")+
   theme_classic() +
   scale_fill_manual(values=c('#2c19d4','#d41948'))


```


**7.** Not only did the researchers randomise whether the sender has a black-sounding name, but they also randomised whether an email had a complex or simple content. Run a regression of complexity on race and interpret your result. Comment on the meaning of this result for the experimental design.

```{r 7}
reg7 <- lm(complexity ~ race, data=data1)
stargazer(reg7, type="text")
summary(reg7)

```

Due to intercept being 0.498, we have mostly a 50% probability of having a complex or simple mail if the race is black. We cannot know. What It stands out is the very low slope of 0.001, which means that a change in race has mostly no influence in the probability of getting a complex mail. That also explains the 0 value of R squared, and due to the high p-value that we have, there is no statistical significance about it. Moreover, in support of this hypothesis, the standard error is 0.5 then the regression line is approximately mistaken half of the times.

**8.** Now regress delay_reply on race and interpret the coefficients in terms of magnitude and statistical significance. Explain why your estimates are likely biased despite having run a clean experiment.

```{r 8}
reg8 <- lm(delay_reply ~ race, data=data)
stargazer(reg8, type="text")
summary(reg8)

```

The p-value is greater than the usual 0.05 alpha level thus we cannot reject the null hypothesis and say coefficients are statistically significant. Moreover, attending to the residual we find that the -27.31 min is closed to the -20.27 median while the max is 933.51, then there is a big magnitude.
On the other hand, the R Squared is almost 0 so the variance of X is not related at all with the result of Y and the standard error of β1 is quite big (0.84 on 1).  The error probably depends on x and that’s why the estimates may be biased.


# 4. MONTE CARLO SIMULATION

```{r m}
# We first define the sample size ( n ) and the number of samples ( rep )
rep <- 1000
n <- 100
beta1 <- 2
# Then we construct the function:
MonteCarloFun <- function(n, beta1)  {
  x <- rnorm(n, 100, 15)
  u <- rnorm(n, 0, 8)
  y <- beta1*x+u
  df <-data.frame(x,y)
  reg <- lm(y~x, data=df)
  beta1coeff <- reg$coefficients[2]
  return(beta1coeff) }
# We replicate the function 1000 times:
MonteCarlo<- replicate(n=rep, expr= MonteCarloFun(n, beta1))
# To construct the confidence interval we use the following:
MonteCarloup <- function(n, beta1)  {
x <- rnorm(n, 100, 15)
u <- rnorm(n, 0, 8)
y <- beta1*x+u
df <-data.frame(x,y)
reg <- lm(y~x, data=df)
beta1coeff <- reg$coefficients[2]
se<-summary(reg)$coef[2,2]
ciup <- beta1coeff + qnorm(0.975)*se

return(ciup) }
# And we replicate it 1000 times:
ciup<- replicate(n=rep, expr= MonteCarloup(n, beta1))

MonteCarlolow <- function(n, beta1)  {
x <- rnorm(n, 100, 15)
u <- rnorm(n, 0, 8)
y <- beta1*x+u
df <-data.frame(x,y)
reg <- lm(y~x, data=df)
beta1coeff <- reg$coefficients[2]
se<-summary(reg)$coef[2,2]

cilow <- beta1coeff - qnorm(0.975)*se
return(cilow) }
cilow<- replicate(n=rep, expr= MonteCarlolow(n, beta1))

ci95<-data.frame(ciup, cilow)


#To calculate the share that include 2:
count<- vector(mode = "numeric", length=1)
count=0
for(j in 1:rep) {
  if (ciup[j]>2) {
    if (cilow[j]<2) {
      count<-count+1
    }
  }
}
share <-count*100/rep
```

In the exercise, it asks us to make a 95% confidence interval and because the result of the experiment gives 94.2 , we can say that the result is correct.
